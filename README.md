ğŸª Arabic Text Summarization with TinyLlama + LoRA
ğŸ“Œ Project Overview
This project explores automatic summarization for Arabic text using TinyLlama (1.1B parameters) fine-tuned with LoRA (Low-Rank Adaptation).
The goal is to efficiently adapt a small language model for Arabic summarization using limited resources.

ğŸ¯ Objectives
Load and clean the XLSum Arabic dataset
Load TinyLlama-1.1B-Chat model
Fine-tune with LoRA for summarization
Evaluate using ROUGE metrics

ğŸ“Š Dataset
Dataset: XLSum (Arabic subset)
~60,000 Arabic article-summary pairs

Example:

Article:

Ø´Ù‡Ø¯Øª Ø§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø© ØªØ·ÙˆØ±Ù‹Ø§ Ù‡Ø§Ø¦Ù„Ù‹Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§...
Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø£Ø¹Ù„Ù†Øª Ø¹Ù† Ø®Ø·Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø±ÙŠÙÙŠØ©...

Summary:

ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ØŒ Ø£Ø«Ø§Ø±Øª Ù‡Ø°Ù‡ Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ØªØ­Ø¯ÙŠØ§Øª Ø¹Ø¯Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø®ØµÙˆØµÙŠØ© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©...
Ø§Ù„Ø­ÙƒÙˆÙ…Ø© ØªØ·Ù„Ù‚ Ø®Ø·Ø© Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø±ÙŠÙÙŠØ©.

ğŸ§  Model & Setup

Base model: TinyLlama/TinyLlama-1.1B-Chat

LoRA: Efficient fine-tuning with minimal memory usage

Quantization: 4-bit (nf4) using bitsandbytes

Libraries:
ğŸ¤— Transformers
ğŸ¤— PEFT
ğŸ¤— TRL
PyTorch

âš™ï¸ Training Details
Batch size: 8
Epochs: 5
Learning rate: 5e-5
Gradient accumulation: 2
Max sequence length: 128 tokens
Fine-tuning method: Supervised Fine-Tuning (SFTTrainer)

ğŸ“ˆ Evaluation
Metric: ROUGE (ROUGE-1, ROUGE-2, ROUGE-L)

Results: Model generated concise summaries with acceptable quality
Example output:
Input:
Ø£Ø¹Ù„Ù†Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ø¹Ù† Ø­Ù…Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªØ·Ø¹ÙŠÙ… Ø¶Ø¯ Ø´Ù„Ù„ Ø§Ù„Ø£Ø·ÙØ§Ù„ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø§Øª...

Generated Summary:
ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© ØªØ·Ù„Ù‚ Ø­Ù…Ù„Ø© ÙˆØ·Ù†ÙŠØ© Ù„Ù„ØªØ·Ø¹ÙŠÙ… Ø¶Ø¯ Ø´Ù„Ù„ Ø§Ù„Ø£Ø·ÙØ§Ù„.

ğŸš€ Challenges
Limited Arabic support in pre-trained LLMs
Prompt understanding in Arabic not always reliable
Hardware/resource constraints

âœ… Conclusion
TinyLlama can be adapted efficiently for Arabic NLP with LoRA
Achieved good summarization quality with limited resources
Provides a foundation for other Arabic NLP tasks

ğŸ”® Future Work
Test larger models (e.g., Mistral, Zephyr)
Explore multi-style summarization (concise, long, analytical)
Conduct human evaluation of summaries

ğŸ“‚ Project Structure
ğŸ“¦ arabic-summarization-tinyllama
 â”£ ğŸ“œ data_preprocessing.py   # Dataset loading & cleaning
 â”£ ğŸ“œ train_lora.py           # Fine-tuning script
 â”£ ğŸ“œ evaluate.py             # ROUGE evaluation
 â”£ ğŸ“œ requirements.txt        # Dependencies
 â”£ ğŸ“œ README.md               # Project overview
 â”— ğŸ“‚ results/                # Sample outputs & metrics

âš¡ Installation & Usage
1. Clone repo
git clone https://github.com/BatoulHamdan/arabic-summarization.git
cd arabic-summarization

2. Install dependencies
pip install -r requirements.txt

3. Run training
python train_lora.py

4. Run evaluation
python evaluate.py

ğŸ™Œ Acknowledgments
XLSum dataset
TinyLlama model
Hugging Face libraries: Transformers, PEFT, TRL

ğŸ”¥ If you like this project, feel free to star â­ the repo and connect with me on LinkedIn.
